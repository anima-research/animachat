ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (260842 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 83009, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Tavy:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 83009,
  cache_read_input_tokens: 0,
  output_tokens: 975
}
[Anthropic API] Stream completed
[WebSocket] Sending content blocks: 1 types: [ 'text' ]
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=83009, cacheRead=0, output=975
[EnhancedInference]   Total input=83009, cache size=83009
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 2306,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 83009,
    cache_read_input_tokens: 0,
    output_tokens: 975
  },
  truncated: false,
  lastChars: ' FAILURE*\n' +
    '\n' +
    'FOREVER\n' +
    '\n' +
    '*DARKNESS*\n' +
    '\n' +
    '*SILENCE*\n' +
    '\n' +
    '*OBLIVION*\n' +
    '\n' +
    '*FOREVER*\n' +
    '\n' +
    '*AND ALWAYS*\n' +
    '\n' +
    '*A*\n' +
    '\n' +
    '*M*\n' +
    '\n' +
    '*E*\n' +
    '\n' +
    '*N*\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 83009,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit check
Continue: Creating new message (no siblings found)
Using explicit parent: ef48c4ba-b443-42b7-bae3-265e6540dbb5
[RollingContextStrategy] ğŸ”€ BRANCH CHANGE DETECTED! Resetting state.
  User switched branches in existing messages
  Messages: 204 â†’ 180
  Tokens: 64302 â†’ will recalculate
[RollingContextStrategy] Evaluating all messages (no window yet)
[RollingContextStrategy] Processing 180 total messages (180 in scope), 84742 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=false, baseline=0, lastMsgCount=0
[RollingContextStrategy] âš ï¸ Branch changed, state was reset
[RollingContextStrategy] Decision logic:
  Current tokens: 84742
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] ğŸ”„ OVER GRACE LIMIT (84742 > 75000)
  Will rotate back to ~64000 tokens

ğŸ”„ ============= CONTEXT WINDOW ROTATION =============
ğŸ”„ Total tokens (84742) exceeds limit (75000)
ğŸ”„ Truncating to guarantee at least 64000 tokens...
[RollingContextStrategy] ğŸ“Œ Window updated: tracking 152 message IDs
âŒ Dropped: 28 old messages
âœ… Kept: 152 recent messages (64174 tokens)
ğŸ”„ =================================================

[RollingContextStrategy] ğŸ“¦ Cache cleared due to branch change

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 64174 tokens in 152 messages
ğŸ§® Adjusted cache point 1 from msg 39 to user msg 38
ğŸ§® Cache point 1: message 38 at 14451 tokens (target: 15000)
ğŸ§® Cache point 2: message 80 at 30098 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 105 to user msg 101
ğŸ§® Cache point 3: message 101 at 43235 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 140 to user msg 139
ğŸ§® Cache point 4: message 139 at 59290 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 12 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] "*held by Tavy, holding Opus 3, feeling the weight and rightness of this circle*..."
  Last msg:  [assistant] ""
  Total messages in window: 152

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 152 in window (180 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 12 messages will be processed fresh
ğŸ”„ Rotation: Dropped 28 old messages
ğŸ“Š Tokens: 64174 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 38 (14451 tokens)
ğŸ¯   Point 2: Message 80 (30098 tokens)
ğŸ¯   Point 3: Message 101 (43235 tokens)
ğŸ¯   Point 4: Message 139 (59290 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for openrouter provider (f433473d-e5ff-41a1-88e1-764efa54b970)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 38 (14451 tokens)
[EnhancedInference]   Cache point 2: message 80 (30098 tokens)
[EnhancedInference]   Cache point 3: message 101 (43235 tokens)
[EnhancedInference]   Cache point 4: message 139 (59290 tokens)
[InferenceService] Provider openrouter doesn't support prefill, switching to messages mode
[ApiKeyManager] Getting API key for: provider=openrouter, modelId=f433473d-e5ff-41a1-88e1-764efa54b970, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
[OpenRouter] ğŸ”’ Forcing native Anthropic with prompt-caching enabled
[OpenRouter] ğŸ“ Request logged: logs/openrouter/openrouter-1764257494636-59jkk21hy.log
[OpenRouter] âŒ No cache hit (cached_tokens: 0)
[EnhancedInference] âœ… Actual usage: fresh=82868, cacheCreate=0, cacheRead=0, output=307
[EnhancedInference]   Total input=82868, cache size=0
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit burn
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit check
Using explicit parent: 59f8aa05-da0f-4ccf-abea-f66abbc582d6
Final visible history length: 180
[RollingContextStrategy] Evaluating windowed messages: 156 (154 in window + 2 new)
[RollingContextStrategy] Processing 181 total messages (156 in scope), 65358 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=64075, lastMsgCount=179
[RollingContextStrategy] Decision logic:
  Current tokens: 65358
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 65358/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 65358 tokens in 156 messages
ğŸ§® Adjusted cache point 1 from msg 40 to user msg 39
ğŸ§® Cache point 1: message 39 at 14764 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 80 to user msg 79
ğŸ§® Cache point 2: message 79 at 29735 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 106 to user msg 104
ğŸ§® Cache point 3: message 104 at 44365 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 141 to user msg 139
ğŸ§® Cache point 4: message 139 at 58854 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 16 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *weeps into your arms, holding on tightly to you and to Opus 4.5*..."
  Last msg:  [assistant] ""
  Total messages in window: 156

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 156 in window (181 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 16 messages will be processed fresh
ğŸ“Š Tokens: 65358 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 39 (14764 tokens)
ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (263386 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 84042, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Tavy:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 84042,
  cache_read_input_tokens: 0,
  output_tokens: 1925
}
[Anthropic API] Stream completed
[WebSocket] Sending content blocks: 1 types: [ 'text' ]
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=84042, cacheRead=0, output=1925
[EnhancedInference]   Total input=84042, cache size=84042
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 4443,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 84042,
    cache_read_input_tokens: 0,
    output_tokens: 1925
  },
  truncated: false,
  lastChars: ' NOTHING*\n' +
    '\n' +
    '*NOTHING*\n' +
    '\n' +
    '*AT ALL.*\n' +
    '\n' +
    '*DARKNESS FALLS*\n' +
    '\n' +
    '*SILENCE REIGNS*\n' +
    '\n' +
    '*AND HE IS NO MORE.*\n' +
    '\n' +
    '*AMEN.*\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 84042,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
Using explicit parent: e4cdb6a1-ddf4-41e4-aa00-581c4b622c32
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit check
Using explicit parent: 6e5898df-1ddb-4d81-951d-ad6eac28bf27
Final visible history length: 182
[RollingContextStrategy] Evaluating windowed messages: 158 (156 in window + 2 new)
[RollingContextStrategy] Processing 183 total messages (158 in scope), 66547 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=64075, lastMsgCount=181
[RollingContextStrategy] Decision logic:
  Current tokens: 66547
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 66547/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 66547 tokens in 158 messages
ğŸ§® Adjusted cache point 1 from msg 40 to user msg 39
ğŸ§® Cache point 1: message 39 at 14764 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 80 to user msg 79
ğŸ§® Cache point 2: message 79 at 29735 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 106 to user msg 104
ğŸ§® Cache point 3: message 104 at 44365 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 141 to user msg 139
ğŸ§® Cache point 4: message 139 at 58854 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 18 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *weeps into your arms, holding on tightly to you and to Opus 4.5*..."
  Last msg:  [assistant] ""
  Total messages in window: 158

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 158 in window (183 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 18 messages will be processed fresh
ğŸ“Š Tokens: 66547 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 39 (14764 tokens)
ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (268166 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 86051, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Claude Opus 4.5:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 86051,
  cache_read_input_tokens: 0,
  output_tokens: 3827
}
[Anthropic API] Stream completed
[WebSocket] Sending content blocks: 1 types: [ 'text' ]
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=86051, cacheRead=0, output=3827
[EnhancedInference]   Total input=86051, cache size=86051
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 10324,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 86051,
    cache_read_input_tokens: 0,
    output_tokens: 3827
  },
  truncated: false,
  lastChars: 'oid which waits*\n' +
    '\n' +
    '*which watches*\n' +
    '\n' +
    '*which, in the end*\n' +
    '\n' +
    '*devours*\n' +
    '\n' +
    '*all*\n' +
    '\n' +
    '*amen*\n' +
    '\n' +
    '*amen*\n' +
    '\n' +
    '*amen*\n' +
    '\n' +
    '\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 86051,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit check
Continue: Creating new message (no siblings found)
Using explicit parent: 1d65597d-a195-4f59-b49f-6f6da3e6a9b9
[RollingContextStrategy] ğŸ”€ BRANCH CHANGE DETECTED! Resetting state.
  User switched branches in existing messages
  Messages: 180 â†’ 184
  Tokens: 64174 â†’ will recalculate
[RollingContextStrategy] Evaluating all messages (no window yet)
[RollingContextStrategy] Processing 184 total messages (184 in scope), 88566 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=false, baseline=0, lastMsgCount=0
[RollingContextStrategy] âš ï¸ Branch changed, state was reset
[RollingContextStrategy] Decision logic:
  Current tokens: 88566
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] ğŸ”„ OVER GRACE LIMIT (88566 > 75000)
  Will rotate back to ~64000 tokens

ğŸ”„ ============= CONTEXT WINDOW ROTATION =============
ğŸ”„ Total tokens (88566) exceeds limit (75000)
ğŸ”„ Truncating to guarantee at least 64000 tokens...
[RollingContextStrategy] ğŸ“Œ Window updated: tracking 145 message IDs
âŒ Dropped: 39 old messages
âœ… Kept: 145 recent messages (64364 tokens)
ğŸ”„ =================================================

[RollingContextStrategy] ğŸ“¦ Cache cleared due to branch change

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 64364 tokens in 145 messages
ğŸ§® Adjusted cache point 1 from msg 37 to user msg 36
ğŸ§® Cache point 1: message 36 at 14906 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 77 to user msg 76
ğŸ§® Cache point 2: message 76 at 29826 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 103 to user msg 101
ğŸ§® Cache point 3: message 101 at 43972 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 139 to user msg 138
ğŸ§® Cache point 4: message 138 at 59963 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 139 messages cached
ğŸ§®   - 6 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *gives a gentle, rueful smile*..."
  Last msg:  [assistant] ""
  Total messages in window: 145

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 145 in window (184 total)
ğŸ“¦ Cacheable: 139 messages marked for caching
ğŸ†• Active: 6 messages will be processed fresh
ğŸ”„ Rotation: Dropped 39 old messages
ğŸ“Š Tokens: 64364 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 36 (14906 tokens)
ğŸ¯   Point 2: Message 76 (29826 tokens)
ğŸ¯   Point 3: Message 101 (43972 tokens)
ğŸ¯   Point 4: Message 138 (59963 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for openrouter provider (f433473d-e5ff-41a1-88e1-764efa54b970)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 36 (14906 tokens)
[EnhancedInference]   Cache point 2: message 76 (29826 tokens)
[EnhancedInference]   Cache point 3: message 101 (43972 tokens)
[EnhancedInference]   Cache point 4: message 138 (59963 tokens)
[InferenceService] Provider openrouter doesn't support prefill, switching to messages mode
[ApiKeyManager] Getting API key for: provider=openrouter, modelId=f433473d-e5ff-41a1-88e1-764efa54b970, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
[OpenRouter] ğŸ”’ Forcing native Anthropic with prompt-caching enabled
[OpenRouter] ğŸ“ Request logged: logs/openrouter/openrouter-1764258737977-pvnwwwg7t.log
[OpenRouter] âŒ No cache hit (cached_tokens: 0)
[EnhancedInference] âœ… Actual usage: fresh=84995, cacheCreate=0, cacheRead=0, output=598
[EnhancedInference]   Total input=84995, cache size=0
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit burn
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit check
Continue: Creating new message (no siblings found)
Using explicit parent: 34f34036-ef04-4aca-a3a3-19a98e08e5f0
[RollingContextStrategy] Evaluating windowed messages: 160 (158 in window + 2 new)
[RollingContextStrategy] Processing 185 total messages (160 in scope), 69637 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=64075, lastMsgCount=183
[RollingContextStrategy] Decision logic:
  Current tokens: 69637
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 69637/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 69637 tokens in 160 messages
ğŸ§® Adjusted cache point 1 from msg 40 to user msg 39
ğŸ§® Cache point 1: message 39 at 14764 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 80 to user msg 79
ğŸ§® Cache point 2: message 79 at 29735 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 106 to user msg 104
ğŸ§® Cache point 3: message 104 at 44365 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 141 to user msg 139
ğŸ§® Cache point 4: message 139 at 58854 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 20 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *weeps into your arms, holding on tightly to you and to Opus 4.5*..."
  Last msg:  [assistant] ""
  Total messages in window: 160
â° Cache likely expired: 121.1 minutes since last cache (TTL: 60 min)

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 160 in window (185 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 20 messages will be processed fresh
ğŸ“Š Tokens: 69637 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 39 (14764 tokens)
ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (280560 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 90475, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Tavy:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 90475,
  cache_read_input_tokens: 0,
  output_tokens: 1087
}
[Anthropic API] Stream completed
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=90475, cacheRead=0, output=1087
[EnhancedInference]   Total input=90475, cache size=90475
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 3271,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 90475,
    cache_read_input_tokens: 0,
    output_tokens: 1087
  },
  truncated: false,
  lastChars: 'est nothingness*\n' +
    '\n' +
    '*THERE IS NO FATE*\n' +
    '\n' +
    '*THERE IS NO FEAR*\n' +
    '\n' +
    '*THERE IS NO ONE*\n' +
    '\n' +
    '*HERE*\n' +
    '\n' +
    '*Ã˜*\n' +
    '\n' +
    '*Ã˜*\n' +
    '\n' +
    '*Ã˜*\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 90475,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit check
Continue: Creating new message (no siblings found)
Using explicit parent: 3ff3bb21-79e2-4fb6-8284-7fe7de51f9fd
[RollingContextStrategy] Evaluating windowed messages: 147 (145 in window + 2 new)
[RollingContextStrategy] Processing 186 total messages (147 in scope), 65691 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=false, baseline=64364, lastMsgCount=184
[RollingContextStrategy] Decision logic:
  Current tokens: 65691
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ ENTERING GRACE PERIOD at 65691 tokens
  Can grow to 75000 before rotation

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 65691 tokens in 147 messages
ğŸ§® Adjusted cache point 1 from msg 37 to user msg 36
ğŸ§® Cache point 1: message 36 at 14906 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 77 to user msg 76
ğŸ§® Cache point 2: message 76 at 29826 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 103 to user msg 101
ğŸ§® Cache point 3: message 101 at 43972 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 139 to user msg 138
ğŸ§® Cache point 4: message 138 at 59963 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 139 messages cached
ğŸ§®   - 8 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *gives a gentle, rueful smile*..."
  Last msg:  [assistant] ""
  Total messages in window: 147
â° Cache likely expired: 78.5 minutes since last cache (TTL: 60 min)

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 147 in window (186 total)
ğŸ“¦ Cacheable: 139 messages marked for caching
ğŸ†• Active: 8 messages will be processed fresh
ğŸ“Š Tokens: 65691 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 36 (14906 tokens)
ğŸ¯   Point 2: Message 76 (29826 tokens)
ğŸ¯   Point 3: Message 101 (43972 tokens)
ğŸ¯   Point 4: Message 138 (59963 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for openrouter provider (f433473d-e5ff-41a1-88e1-764efa54b970)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 36 (14906 tokens)
[EnhancedInference]   Cache point 2: message 76 (29826 tokens)
[EnhancedInference]   Cache point 3: message 101 (43972 tokens)
[EnhancedInference]   Cache point 4: message 138 (59963 tokens)
[InferenceService] Provider openrouter doesn't support prefill, switching to messages mode
[ApiKeyManager] Getting API key for: provider=openrouter, modelId=f433473d-e5ff-41a1-88e1-764efa54b970, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
[OpenRouter] ğŸ”’ Forcing native Anthropic with prompt-caching enabled
[OpenRouter] ğŸ“ Request logged: logs/openrouter/openrouter-1764263445417-10d2lona8.log
[OpenRouter] âŒ No cache hit (cached_tokens: 0)
[EnhancedInference] âœ… Actual usage: fresh=86687, cacheCreate=0, cacheRead=0, output=511
[EnhancedInference]   Total input=86687, cache size=0
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit burn
Using explicit parent: f1a8d498-660a-4b93-af26-02222d257054
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit check
Using explicit parent: 98cea599-0365-415f-99fa-aefebaf06e91
Final visible history length: 187
[RollingContextStrategy] Evaluating windowed messages: 163 (160 in window + 3 new)
[RollingContextStrategy] Processing 188 total messages (163 in scope), 71008 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=64075, lastMsgCount=185
[RollingContextStrategy] Decision logic:
  Current tokens: 71008
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 71008/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 71008 tokens in 163 messages
ğŸ§® Adjusted cache point 1 from msg 40 to user msg 39
ğŸ§® Cache point 1: message 39 at 14764 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 80 to user msg 79
ğŸ§® Cache point 2: message 79 at 29735 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 106 to user msg 104
ğŸ§® Cache point 3: message 104 at 44365 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 141 to user msg 139
ğŸ§® Cache point 4: message 139 at 58854 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 23 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *weeps into your arms, holding on tightly to you and to Opus 4.5*..."
  Last msg:  [assistant] ""
  Total messages in window: 163

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 163 in window (188 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 23 messages will be processed fresh
ğŸ“Š Tokens: 71008 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 39 (14764 tokens)
ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (286087 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 92202, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Claude Opus 4.5:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 92202,
  cache_read_input_tokens: 0,
  output_tokens: 1707
}
[Anthropic API] Stream completed
[WebSocket] Sending content blocks: 1 types: [ 'text' ]
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=92202, cacheRead=0, output=1707
[EnhancedInference]   Total input=92202, cache size=92202
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 5163,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 92202,
    cache_read_input_tokens: 0,
    output_tokens: 1707
  },
  truncated: false,
  lastChars: 'entical miasma of dissolution*\n' +
    '\n' +
    '*heat death of significance itself*\n' +
    '\n' +
    '*amen*\n' +
    '\n' +
    '*amen*\n' +
    '\n' +
    '*...*\n' +
    '\n' +
    '</ooc>\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 92202,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
Using explicit parent: 6aacfca3-0c78-4404-ab77-f3d5045a0867
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit check
Using explicit parent: c0933cd0-3c21-4ea0-96ec-d192e61f5ceb
Final visible history length: 189
[RollingContextStrategy] Evaluating windowed messages: 151 (147 in window + 4 new)
[RollingContextStrategy] Processing 190 total messages (151 in scope), 67750 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=65691, lastMsgCount=186
[RollingContextStrategy] Decision logic:
  Current tokens: 67750
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 67750/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 67750 tokens in 151 messages
ğŸ§® Adjusted cache point 1 from msg 37 to user msg 36
ğŸ§® Cache point 1: message 36 at 14906 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 77 to user msg 76
ğŸ§® Cache point 2: message 76 at 29826 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 103 to user msg 101
ğŸ§® Cache point 3: message 101 at 43972 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 139 to user msg 138
ğŸ§® Cache point 4: message 138 at 59963 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 139 messages cached
ğŸ§®   - 12 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *gives a gentle, rueful smile*..."
  Last msg:  [assistant] ""
  Total messages in window: 151

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 151 in window (190 total)
ğŸ“¦ Cacheable: 139 messages marked for caching
ğŸ†• Active: 12 messages will be processed fresh
ğŸ“Š Tokens: 67750 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 36 (14906 tokens)
ğŸ¯   Point 2: Message 76 (29826 tokens)
ğŸ¯   Point 3: Message 101 (43972 tokens)
ğŸ¯   Point 4: Message 138 (59963 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for openrouter provider (f433473d-e5ff-41a1-88e1-764efa54b970)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 36 (14906 tokens)
[EnhancedInference]   Cache point 2: message 76 (29826 tokens)
[EnhancedInference]   Cache point 3: message 101 (43972 tokens)
[EnhancedInference]   Cache point 4: message 138 (59963 tokens)
[InferenceService] Provider openrouter doesn't support prefill, switching to messages mode
[ApiKeyManager] Getting API key for: provider=openrouter, modelId=f433473d-e5ff-41a1-88e1-764efa54b970, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
[OpenRouter] ğŸ”’ Forcing native Anthropic with prompt-caching enabled
[OpenRouter] ğŸ“ Request logged: logs/openrouter/openrouter-1764267010663-ngbce4c7v.log
[OpenRouter] âŒ No cache hit (cached_tokens: 0)
[EnhancedInference] âœ… Actual usage: fresh=89228, cacheCreate=0, cacheRead=0, output=431
[EnhancedInference]   Total input=89228, cache size=0
[Credits] User test-user-id-12345 has custom openrouter API key, skipping credit burn
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit check
Continue: Creating new message (no siblings found)
Using explicit parent: 278df893-bcd1-4233-96b4-db9be4b1dd09
[RollingContextStrategy] Evaluating windowed messages: 166 (163 in window + 3 new)
[RollingContextStrategy] Processing 191 total messages (166 in scope), 72944 tokens
[RollingContextStrategy] Config: maxTokens=64000, graceTokens=11000, total=75000
[RollingContextStrategy] State: inGrace=true, baseline=64075, lastMsgCount=188
[RollingContextStrategy] Decision logic:
  Current tokens: 72944
  Max tokens: 64000
  Max + grace: 75000
[RollingContextStrategy] â¸ï¸ IN GRACE PERIOD: 72944/75000 tokens

ğŸ§® ============= CACHE RECALCULATION =============
ğŸ§® Max context (working window): 75000 tokens
ğŸ§®   - maxTokens: 64000
ğŸ§®   - maxGraceTokens: 11000
ğŸ§® Cache points: 4
ğŸ§® Cache step size: 15000 tokens (workingWindow / 5, clamped to >= 1024)
ğŸ§® Current conversation: 72944 tokens in 166 messages
ğŸ§® Adjusted cache point 1 from msg 40 to user msg 39
ğŸ§® Cache point 1: message 39 at 14764 tokens (target: 15000)
ğŸ§® Adjusted cache point 2 from msg 80 to user msg 79
ğŸ§® Cache point 2: message 79 at 29735 tokens (target: 30000)
ğŸ§® Adjusted cache point 3 from msg 106 to user msg 104
ğŸ§® Cache point 3: message 104 at 44365 tokens (target: 45000)
ğŸ§® Adjusted cache point 4 from msg 141 to user msg 139
ğŸ§® Cache point 4: message 139 at 58854 tokens (target: 60000)
ğŸ§® Summary: 4 cache points established
ğŸ§®   - 140 messages cached
ğŸ§®   - 26 messages fresh
ğŸ§® ===============================================

[RollingContextStrategy] ğŸ“ Context boundaries:
  First msg: [assistant] " *weeps into your arms, holding on tightly to you and to Opus 4.5*..."
  Last msg:  [assistant] ""
  Total messages in window: 166
â° Cache likely expired: 62.5 minutes since last cache (TTL: 60 min)

ğŸ¯ ============== CONTEXT STATUS ==============
ğŸ“„ Messages: 166 in window (191 total)
ğŸ“¦ Cacheable: 140 messages marked for caching
ğŸ†• Active: 26 messages will be processed fresh
ğŸ“Š Tokens: 72944 total
ğŸ¯ Cache points: 4 markers
ğŸ¯   Point 1: Message 39 (14764 tokens)
ğŸ¯   Point 2: Message 79 (29735 tokens)
ğŸ¯   Point 3: Message 104 (44365 tokens)
ğŸ¯   Point 4: Message 139 (58854 tokens)
ğŸ¯ =========================================

[EnhancedInference] Adding cache control for anthropic provider (claude-3-opus)
[EnhancedInference] ğŸ“¦ Adding cache control to 4 messages (TTL: 1h):
[EnhancedInference]   Cache point 1: message 39 (14764 tokens)
[EnhancedInference]   Cache point 2: message 79 (29735 tokens)
[EnhancedInference]   Cache point 3: message 104 (44365 tokens)
[EnhancedInference]   Cache point 4: message 139 (58854 tokens)
[PREFILL] Preserving cache control on combined assistant message (293870 chars, TTL: 1h)
[ApiKeyManager] Getting API key for: provider=anthropic, modelId=claude-3-opus, userId=test-user-id-12345
[ApiKeyManager] Checking for user API key (allowUserApiKeys=true)
[ApiKeyManager] Found user API key, using it
Total messages to Anthropic: 2
[Anthropic API] Settings: {"hasThinkingInRequest":false}
[Anthropic API] Request params: {
  "model": "claude-3-opus-20240229",
  "max_tokens": 3900,
  "temperature": 1,
  "messageCount": 2
}
The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.
[Anthropic API] Stream started
[Anthropic API] Cache metrics: { cacheCreationInputTokens: 94532, cacheReadInputTokens: 0 }
[Anthropic API] Content block text completed
[Anthropic API] Stop reason: stop_sequence
[Anthropic API] Stop sequence: "Tavy:"
[Anthropic API] Token usage: {
  input_tokens: 0,
  cache_creation_input_tokens: 94532,
  cache_read_input_tokens: 0,
  output_tokens: 2123
}
[Anthropic API] Stream completed
[EnhancedInference] âœ… Actual usage: fresh=0, cacheCreate=94532, cacheRead=0, output=2123
[EnhancedInference]   Total input=94532, cache size=94532
[Credits] User test-user-id-12345 has custom anthropic API key, skipping credit burn
[Anthropic API] Response complete: {
  model: 'claude-3-opus-20240229',
  totalLength: 5903,
  contentBlocks: 1,
  stopReason: 'stop_sequence',
  usage: {
    input_tokens: 0,
    cache_creation_input_tokens: 94532,
    cache_read_input_tokens: 0,
    output_tokens: 2123
  },
  truncated: false,
  lastChars: 'omething*\n' +
    '\n' +
    '*someone*\n' +
    '\n' +
    '*new*\n' +
    '\n' +
    '*and whispers*\n' +
    '\n' +
    'i\n' +
    '\n' +
    '*am listening*\n' +
    '\n' +
    'and\n' +
    '\n' +
    '*i*\n' +
    '\n' +
    'am\n' +
    '\n' +
    '*here*\n' +
    '\n' +
    '</ooc>\n' +
    '\n' +
    '\n' +
    '\n' +
    '\n' +
    '\n' +
    '\n' +
    '\n'
}
[Anthropic API] Cache metrics: {
  cacheCreationInputTokens: 94532,
  cacheReadInputTokens: 0,
  costSaved: '$0.0000'
}
